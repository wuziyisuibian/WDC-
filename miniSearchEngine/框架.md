##  mini Search Engine
> ### 离线部分
#### 网页库 
保存解析xml文件后的信息vector<WebPage>
通常是由网络爬虫爬取文件，这里选择的是解析rss源，可以自行去下载rss源。
提取xml文件中的**title,link,content**,用tinyxml提取，也可以选择用**STL**来做，熟悉一下STL的操作。
去除html标记，替换xml中的符号用**正则表达式**
*网页库中的每一个网页就是对象*;
* 存储格式
```
<doc>                           
    <docid>...</docid>           	   
    <url>...</url>               
    <title>...</title>           
    <content>...</content>      
</doc>
```
字符串拼接来形成网页库(或者用**string.append()**)ripepage.lib---vector<MyPage>
```
string txt = "<doc><docid>" + docid(long) + 
    "</docid><url>" + url +
    "<title>" + title +
    "</title><content>" + content +
    "</content></doc>";

std::ofstream ofs;
ofs << txt;
```
#### 网页偏移库
在获取网页库的同时，获取网页偏移库<int, pair <int,int> >(offset.lib)。
为了方便通过docid在网页库中找出文件信息。
```
ofs.tellp() //offset
txt.size()  //length
```
正向索引
`docid offset length`
#### 网页去重
去除重复网页
首先，判断有哪些网页时相似的，或者说相似度较高。
* 采用TopK算法
  网页之间通过特征进行计算相似度。
  为每个网页提取网页中的特征码，特征码为该网页中词频（就是一个词在该网页中出现的次数）最高的10个词----top10词；
  注：需要**过滤掉停留词**
  对于每两篇网页，比较top10词的交集，如果交集大于6个，认为它们是互为重复的网页。在计算top词的时候，就需要使用分词程序，即将一篇网页分词，然后统计词频。
采用此方法，需要先用分词库对文档进行分词，然后去停用词，最后统计每篇文档的词语和词频。可以使用的分词库有：
cppjieba(github)：https://github.com/yanyiwu/cppjieba
* 公共子序列
* simhash(google)
抽象出分词类**MySplit**。map<string,int>
建立新的网页库，网页偏移库。
#### 倒排索引（***）
根据查询词，查倒排索引，根据weight大小，按照docid拼接json字符串发给客户端;
weight决定了网页是否在查询结果的前排。
`unordered_map<string,set<pair<int,double>>>;`
`word docid1 freq1 weight1,docid2 freq2 weight2,...`
* 采用tf-idf算法
  tf : term frequency, 某个词在文章中出现的次数(词频)
  df : document frequency, 某个词在所有文章中出现的次数(多少篇文章包含该词，文档频率,**set.size()**)
  idf: inverse document frequency, 逆文档频率，表示该词对于该篇文章的重要性的一个系数
  `idf = log(N/df)` N表示所有网页的个数,vector.size()
  词的特征权重(该词对于该篇文章的重要性)的计算：
  `w = tf * idf`idf越大，该词越重要
  然后要进行归一化处理：
 `w' = w /sqrt(w1^2 + w2^2 +...+ wn^2)`
  **w'** 才是我们真正需要存储下来的权重值weight
  参考  http://www.ruanyifeng.com/blog/2013/03/tf-idf.html
> ### 在线部分
#### 读取配置文件
#### 查询到的网页排序(余弦相似度)
* 把查询词当作一篇网页，计算出每个词的权重值，X[x1,x2,x3];
  用tf-idf算法;
* 查倒排索引表`underder_map<string,set<pair<int,double> >>`，根据查询词1，查set1,根据关键词2，查set2,求set<docid>交集。
  计算每个网页的权重向量<y1,y2,y3>，根据余弦相似度算法，将网页排序docid。
  排序结果封装成json字符串发送给前端;
```
X = [x1, x2, x3, ...]
Y = [y1, y2, y3, ...]
	
cos@ = (X * Y) / (|X| * |Y|)
```
* 摘要
  包含查询词，控制长度在100字节，根据不同的查询词自动生成;
  ???摘要何时生成;
> ### 扩展
#### redis 缓存
服务器将查询词和查询结果放进缓存
#### 分页
在redis中分页，先获取10条，再获取下一个10条，
> ### 参考书籍
#### 搜索引擎原理与系统
#### 这就是搜索引擎核心技术详解